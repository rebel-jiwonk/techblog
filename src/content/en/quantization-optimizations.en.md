---
title: "Optimizing Quantization for Low-Power NPU Inference"
slug: "quantization-optimizations"
description: "Explore how fine-tuned quantization boosts performance on Rebellions NPUs."
date: "2024-06-10"
lang: "en"
tags: ["Quantization", "Performance"]
authors: ["Kim Hee-jun"]
---

Static and dynamic quantization can reduce model size and inference latency significantly.  
On Rebellions NPUs, post-training quantization (PTQ) is supported via both PyTorch and Hugging Face pipelines.

Make sure to benchmark models with representative datasets to avoid accuracy drop.